{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae4041e2-e908-4e65-80f9-07ee2d12e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import cssutils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from flashtext import KeywordProcessor\n",
    "from ftlangdetect import detect\n",
    "from langcodes import Language\n",
    "from typing import Union\n",
    "\n",
    "cssutils.log.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7fec8d9-ef61-4e9f-a0e2-d0f6d847cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/html_full\"\n",
    "files = [file for file in os.listdir(path) if file.endswith(\".html\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77c4bb0b-163b-4f78-9ce5-014cc62bef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_number = {\n",
    "                    'product_info': ('1.', '2.'),\n",
    "                    'composition': ('2.', '3.'),\n",
    "                    'indications': ('4.1', '4.2'),\n",
    "                    'posology': ('4.2', '4.3'),\n",
    "                    'contraindications': ('4.3', '4.4'),\n",
    "                    'special_warnings': ('4.4', '4.5'),\n",
    "                    'pregnancy': ('4.5', '4.6'),\n",
    "                    'driving': ('4.6', '4.7'),\n",
    "                    'side_effects': ('4.7', '4.8'),\n",
    "                     'shelf_life': ('6.3', '6.4'),\n",
    "                     'storage': ('6.4', '6.5'),\n",
    "                     'package': ('6.5', '6.6'),\n",
    "                     'marketing_authorization_numbers': ('8.', '9.')\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b401dff-20ee-4bd5-abe4-575d9469a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(soup):\n",
    "    headers = []\n",
    "    for div in soup.find_all(\"div\"):\n",
    "        try:\n",
    "            headers.append(div.find(\"p\").get_text().replace(\"&nbsp;\", \" \").replace(\"\\xa0\", \" \")\\\n",
    "        .replace(\"\", \"\").replace(\"•\", \"\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if len(list(set(headers))) == 1 and headers[0].strip():\n",
    "        return headers[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_bold_classes(styles):\n",
    "    bold_classes = []\n",
    "    for style in styles:\n",
    "        css = cssutils.parseString(style.encode_contents())\n",
    "        for rule in css:\n",
    "            if rule.type == rule.STYLE_RULE:\n",
    "                style = rule.selectorText\n",
    "                for item in rule.style:\n",
    "                    if item.name == \"font-weight\" and item.value == 'bold':\n",
    "                        bold_classes.append(style.lstrip(\".\"))\n",
    "    return bold_classes\n",
    "\n",
    "def filter_headings(text, level=1):\n",
    "    \"\"\"\n",
    "    level1: match the headings with numbering patterns: startswith a digit followed by '.'\n",
    "    level2: match the headings with starting with only digit (applicable for section 1, 8 and 9)\n",
    "    \n",
    "    Steps:\n",
    "    1. match with regex\n",
    "    2. match the title start and end\n",
    "    3. return the matched standardized section title and start, end headings\n",
    "    \"\"\"\n",
    "    text = text.replace(\"&nbsp;\", \" \").replace(\"\\xa0\", \" \").replace(\"<br/>\", \"\").strip()\n",
    "\n",
    "    if level == 1:\n",
    "        regex = \"^\\d\\.\"\n",
    "    \n",
    "    elif level == 2:\n",
    "        regex = \"^\\d\"\n",
    "\n",
    "    if re.match(regex, text):\n",
    "        titles = []\n",
    "        positions = []\n",
    "\n",
    "        for title, numbers in entity_to_number.items():\n",
    "            if level == 2 and title not in [\"product_info\", \"marketing_authorization_numbers\", \"composition\"]:\n",
    "                continue\n",
    "            \n",
    "            if level == 1:\n",
    "                start_text = numbers[0]\n",
    "                end_text = numbers[1]\n",
    "\n",
    "            elif level == 2:\n",
    "                start_text = numbers[0].strip(\".\")\n",
    "                end_text = numbers[1].strip(\".\")\n",
    "\n",
    "            if text.startswith(start_text):\n",
    "                titles.append(title)\n",
    "                positions.append(\"start\")\n",
    "                \n",
    "            if text.startswith(end_text):\n",
    "                titles.append(title)\n",
    "                positions.append(\"end\")\n",
    "    \n",
    "        return titles, positions\n",
    "    \n",
    "    return [], []\n",
    "\n",
    "def extract_sections(sections, document_text, heading_detection=True):\n",
    "    for section, position in sections.items():\n",
    "        extract = False\n",
    "        content = \"\"\n",
    "        start_point = 0\n",
    "\n",
    "        if \"start\" in position and \"end\" in position:\n",
    "            start_text = position[\"start\"].get_text(separator=\"\\n\").replace(\"&nbsp;\", \" \").replace(\"\\xa0\", \" \").strip().split(\"\\n\")[0].strip()\n",
    "            end_text = position[\"end\"].get_text(separator=\"\\n\").replace(\"&nbsp;\", \" \").replace(\"\\xa0\", \" \").strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "            # print(start_text, end_text)\n",
    "\n",
    "            if len(start_text) > 4:\n",
    "                start_point += 1\n",
    "\n",
    "            # extract after beginning of section till end\n",
    "            for i, line in enumerate(document_text[1:]):\n",
    "                if extract:\n",
    "                    start_point += 1\n",
    "\n",
    "                if extract and end_text == line.strip():\n",
    "                    extract = False\n",
    "                    break\n",
    "\n",
    "                if extract and start_point > 1 and line.strip():\n",
    "                    if line == \"-->\" or line == \"<!--\" or (line.__contains__(\"margin\") and line.__contains__(\"padding\"))\\\n",
    "                    or (line.__contains__(\"font-size\") and line.__contains__(\"font-family\"))\\\n",
    "                    or ((len(line) - line.count(\" \")) / len(line)) < 0.3 or len(line) < 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        content += f\"\\n{line}\"\n",
    "\n",
    "                    # if ((len(line) - line.count(\" \")) / len(line)) < 0.3 or len(line) < 3:\n",
    "                    #     print(line)\n",
    "\n",
    "                if start_text == line.strip():\n",
    "                    extract = True\n",
    "\n",
    "                # if start_text == \"6.3\" and extract:\n",
    "                #     print(extract, start_point, line, content)\n",
    "                \n",
    "            content = content.lstrip(\"\\n\").strip()\n",
    "\n",
    "            # remove headings if they're split from numbers\n",
    "            # print(start_text, content)\n",
    "            if len(start_text) <=4:\n",
    "                content = \"\\n\".join(content.split(\"\\n\")[1:])\n",
    "\n",
    "        elif not heading_detection:\n",
    "            start_text = entity_to_number[section][0]\n",
    "            end_text = entity_to_number[section][1]\n",
    "            \n",
    "            for i, line in enumerate(document_text[1:]):\n",
    "                if extract:\n",
    "                    start_point += 1\n",
    "\n",
    "                if extract and line.strip().startswith(end_text):\n",
    "                    extract = False\n",
    "                    break\n",
    "\n",
    "                if extract and start_point > 0 and line.strip():\n",
    "                    if line == \"-->\" or line == \"<!--\" or (line.__contains__(\"margin\") and line.__contains__(\"padding\"))\\\n",
    "                    or (line.__contains__(\"font-size\") and line.__contains__(\"font-family\"))\\\n",
    "                    or ((len(line) - line.count(\" \")) / len(line)) < 0.3 or len(line) < 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        content += f\"\\n{line}\"\n",
    "\n",
    "                    # if ((len(line) - line.count(\" \")) / len(line)) < 0.3 or len(line) < 3:\n",
    "                    #     print(line)\n",
    "\n",
    "                if line.strip().startswith(start_text):\n",
    "                    extract = True\n",
    "            \n",
    "            content = content.lstrip(\"\\n\").strip()\n",
    "            # print(start_text, end_text, content)\n",
    "\n",
    "        if content == \"\":\n",
    "            missing_sections_count[section].append(file)\n",
    "\n",
    "        sections[section][\"content\"] = content\n",
    "        all_contents[section] = content\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_language(text):\n",
    "    result = detect(text=text, low_memory=True)\n",
    "    return Language.get(result[\"lang\"]).display_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "187e5de3-a968-4406-9211-a55cbe1d2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "full_data = []\n",
    "failed_docs = []\n",
    "export_data = []\n",
    "\n",
    "files_to_check = []\n",
    "\n",
    "# TODO: Check how many docs using Level 2, keep track of them and analyse if that's producing False Positives\n",
    "\n",
    "l2 = []\n",
    "\n",
    "missing_sections_count = {\n",
    "            'product_info': [],\n",
    "            'composition': [],\n",
    "            'indications': [],\n",
    "            'posology': [],\n",
    "            'contraindications': [],\n",
    "            'special_warnings': [],\n",
    "            'pregnancy': [],\n",
    "            'driving': [],\n",
    "            'side_effects': [],\n",
    "            'shelf_life': [],\n",
    "            'storage': [],\n",
    "            'package': [],\n",
    "            'marketing_authorization_numbers': []\n",
    "            }\n",
    "\n",
    "detected_headers = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    # if file not in files_to_check:\n",
    "    #     continue\n",
    "\n",
    "    sections = {'product_info': {},\n",
    "                'composition': {},\n",
    "                'indications': {},\n",
    "                'posology': {},\n",
    "                'contraindications': {},\n",
    "                'special_warnings': {},\n",
    "                'pregnancy': {},\n",
    "                'driving': {},\n",
    "                'side_effects': {},\n",
    "                'shelf_life': {},\n",
    "                'storage': {},\n",
    "                'package': {},\n",
    "                'marketing_authorization_numbers': {}\n",
    "               }\n",
    "    all_contents = {}\n",
    "    \n",
    "    # Parse Document\n",
    "    with open(os.path.join(path, file)) as fp:\n",
    "        soup = BeautifulSoup(fp, 'html.parser')\n",
    "           \n",
    "        # get document text as string\n",
    "        document_text = soup.find().get_text(separator='\\n').replace(\"&nbsp;\", \" \").replace(\"\\xa0\", \" \")\\\n",
    "        .replace(\"\", \"\").replace(\"•\", \"\").split(\"\\n\")\n",
    "        \n",
    "        # detect headers using repeated first paragraph text in all <div>\n",
    "        header = get_header(soup)\n",
    "\n",
    "        # remove headers from the document text\n",
    "        if header:\n",
    "            detected_headers.append(header)\n",
    "            document_text = [text for text in document_text if text != header]\n",
    "\n",
    "        styles = soup.select('style')\n",
    "\n",
    "        # get bold class using css data\n",
    "        bold_classes = get_bold_classes(styles)\n",
    "\n",
    "        # get headings using heading tag\n",
    "        headings = soup.find_all(re.compile(\"^h[1-6]$\"))\n",
    "\n",
    "        if len(headings) > 20:\n",
    "            pass\n",
    "            # for heading in headings:\n",
    "            #     print(heading.get_text())\n",
    "        else:\n",
    "            # get headings using bold tag\n",
    "            headings = soup.find_all(lambda t: t.name == 'b')\n",
    "            if len(headings) > 20:\n",
    "                pass\n",
    "                # for heading in headings:\n",
    "                #     print(heading.get_text())\n",
    "            else:\n",
    "                # get headings using bold flag using css classes\n",
    "                headings = []\n",
    "                for s in soup.find_all():\n",
    "                    if s.name == 'span':\n",
    "                        parent = s.find_parent()\n",
    "                        if parent and parent.get(\"class\"):\n",
    "                            if parent.get(\"class\")[0] in bold_classes:\n",
    "                                headings.append(s)\n",
    "    if headings:\n",
    "        for heading in headings:\n",
    "            # filter headings with numberings\n",
    "            matches, positions = filter_headings(heading.get_text(), level=1)\n",
    "            if matches:\n",
    "                for match, position in zip(matches, positions):\n",
    "                    if not position in sections[match]:\n",
    "                        sections[match][position] = heading\n",
    "\n",
    "        if (not \"start\" in sections[\"product_info\"] or not \"end\" in sections[\"product_info\"]\\\n",
    "           or not \"start\" in sections[\"composition\"] or not \"end\" in sections[\"composition\"]\\\n",
    "            or not \"start\" in sections[\"marketing_authorization_numbers\"]\\\n",
    "           or not \"end\" in sections[\"marketing_authorization_numbers\"])\\\n",
    "        and any([sections[\"indications\"], sections[\"shelf_life\"], sections[\"storage\"], sections[\"package\"]]):\n",
    "            for heading in headings:\n",
    "                # filter headings with numberings without '.'\n",
    "                matches, positions = filter_headings(heading.get_text(), level=2)\n",
    "                if matches:\n",
    "                    for match, position in zip(matches, positions):\n",
    "                        if not position in sections[match]:\n",
    "                            # print(f\"{file} level 2 match found for {match}, {position}, {heading}\")\n",
    "                            l2.append(file)\n",
    "                            sections[match][position] = heading\n",
    "            \n",
    "        # print(sections)\n",
    "        # extract content of the section\n",
    "        sections = extract_sections(sections, document_text)\n",
    "        \n",
    "        # identify black triangle\n",
    "        triangle_flag = False\n",
    "        if \"start\" in sections[\"product_info\"]:\n",
    "            section_start = sections[\"product_info\"][\"start\"].get_text(separator=\"\\n\").replace(\"&nbsp;\", \" \").replace(\"\\xa0\", \" \").strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "            for i, line in enumerate(document_text[1:]):\n",
    "                if line.strip().startswith(section_start):\n",
    "                    break\n",
    "            \n",
    "            if i < 300:\n",
    "                pre_section = \"\"\n",
    "                for line in document_text[1:i]:\n",
    "                    if line == \"-->\" or line == \"<!--\" or line.__contains__(\"scaleY\") or line.__contains__(\"scaleX\") or line.__contains__(\"flip\") or (line.__contains__(\"margin\") and line.__contains__(\"padding\")) or (line.__contains__(\"font-size\") and line.__contains__(\"font-family\")):\n",
    "                        pass\n",
    "                    elif line.strip():\n",
    "                        pre_section += f\"\\n{line}\"\n",
    "\n",
    "                if \"4.8\" in pre_section:\n",
    "                    triangle_flag = True\n",
    "            \n",
    "        # print(all_contents)\n",
    "        sections[\"filename\"] = file\n",
    "        full_data.append(sections)\n",
    "        \n",
    "        all_contents[\"filename\"] = file\n",
    "        # all_contents[\"triangle_flag\"] = triangle_flag\n",
    "        export_data.append(all_contents)\n",
    "        \n",
    "        if file in files_to_check:\n",
    "            # print(headings)\n",
    "            print(file, triangle_flag)\n",
    "            \n",
    "\n",
    "    else:\n",
    "        triangle_flag = \"UNKNOWN\"\n",
    "        \n",
    "        # logic for docs without proper heading formatting\n",
    "        sections = extract_sections(sections, document_text, heading_detection=False)\n",
    "        \n",
    "        failed_docs.append(file)\n",
    "        sections[\"filename\"] = file\n",
    "        full_data.append(sections)\n",
    "\n",
    "        all_contents[\"filename\"] = file\n",
    "        # all_contents[\"triangle_flag\"] = triangle_flag\n",
    "        export_data.append(all_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbda0c0e-04c5-42eb-9b45-eae46ba6a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies = {\n",
    "            \"snomed\": \"data/ontologies/SNOMED.csv\",\n",
    "            \"ncit\": \"data/ontologies/NCIT.csv\",\n",
    "            \"rxnorm\": \"data/ontologies/RXNORM.csv\",\n",
    "            \"loinc\": \"data/ontologies/LOINC.csv\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cbf5599-1efa-4dca-b96c-8585a63a473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioOntologyParser():\n",
    "    def __init__(self, name, in_path):\n",
    "        self.name = name\n",
    "        self.in_path = in_path\n",
    "\n",
    "    _columns_to_use = [\"Class ID\", \"Preferred Label\", \"Synonyms\"]\n",
    "\n",
    "    _columns_drop_na = [\"Class ID\", \"Preferred Label\"]\n",
    "\n",
    "    def convert_to_list(self, x: Union[float, str], delimiter: str = \";\") -> list:\n",
    "        return x.split(delimiter) if not pd.isna(x) else []\n",
    "\n",
    "    def curate_synonyms(self, synonyms: list, delimiter: str = \";\") -> list:\n",
    "        all_synonyms = []\n",
    "        for synonym in synonyms:\n",
    "            all_synonyms.extend(self.convert_to_list(synonym, delimiter=delimiter))\n",
    "        return all_synonyms\n",
    "\n",
    "    def parse_to_dict(self) -> pd.DataFrame:\n",
    "        data = pd.read_csv(self.in_path, usecols=self._columns_to_use)\n",
    "\n",
    "        data.dropna(subset=self._columns_drop_na, inplace=True)\n",
    "        data.replace(\"&#x7C;\", \"|\", inplace=True, regex=True)\n",
    "\n",
    "        if self.name == \"ncit\":\n",
    "            data[\"Class ID\"].replace(\"Thesaurus.owl#\", \"\", inplace=True, regex=True)\n",
    "\n",
    "        data[\"id\"] = data[\"Class ID\"].apply(lambda x: str(x).split(\"/\")[-1])\n",
    "\n",
    "        data[\"synonyms\"] = data.apply(\n",
    "            lambda x: [x[\"Preferred Label\"]] + self.curate_synonyms([x.Synonyms], delimiter=\"|\"),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        vocab = {}\n",
    "        labels = {}\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            id_ = row[\"id\"]\n",
    "            default_label = row[\"Preferred Label\"]\n",
    "            labels[id_] = default_label\n",
    "            \n",
    "            all_syns = [default_label]\n",
    "            all_syns.extend(row[\"synonyms\"])\n",
    "            vocab[id_] = all_syns\n",
    "            \n",
    "        return vocab, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecaa2a-1b1c-4640-b7bf-4937c9cb5bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "keywords = {}\n",
    "labels = {}\n",
    "taggers = {}\n",
    "\n",
    "for ontology, ontology_path in tqdm(ontologies.items()):\n",
    "    parser = BioOntologyParser(ontology, ontology_path)\n",
    "    keywords[ontology], labels[ontology] = parser.parse_to_dict()\n",
    "    \n",
    "    temp_tagger = KeywordProcessor()\n",
    "    temp_tagger.add_keywords_from_dict(keywords[ontology])\n",
    "    taggers[ontology] = temp_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863b588-de96-430b-8eea-54af15190f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_tagger(contents):\n",
    "    extraction = {}\n",
    "    for key, value in contents.items():\n",
    "        if key == \"filename\":\n",
    "            continue\n",
    "\n",
    "        for ontology, tagger in taggers.items():\n",
    "            entities = tagger.extract_keywords(value)\n",
    "            output = [(entity, labels[ontology][entity]) for entity in entities]\n",
    "            extraction[f\"{ontology}.{key}\"] = output\n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2775c-6607-4ce1-bf06-d77c904bc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_data = []\n",
    "\n",
    "for sample in export_data:\n",
    "    document = sample.copy()\n",
    "    nlp_output = nlp_tagger(document)\n",
    "    \n",
    "    document.update(nlp_output)\n",
    "    enriched_data.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c8a02-e754-4816-a485-5c2e3c6f9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/output/raw_export.json\", 'w') as fp:\n",
    "    json.dump(export_data, fp)\n",
    "    \n",
    "with open(\"data/output/enriched_export.json\", 'w') as fp:\n",
    "    json.dump(enriched_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3a550-70ab-4d84-a820-ce26b54db5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
